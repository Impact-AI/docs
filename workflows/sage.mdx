---
title: 'Test'
description: 'Automated and exploratory testing framework for AI quality assurance'
icon: 'check'
---

# Test (Sage)

Sage is your testing and quality-assurance agent.  It orchestrates **automatic regression tests, exploratory stress scenarios, and human-in-the-loop reviews** to make sure every AI release meets user expectations and compliance requirements before it reaches production.

## Testing flavours

| Flavour | When to use | Example |
|---------|-------------|---------|
| **Regression** | Validate that previously working behaviour still works after a change | New prompt variation shouldn’t break existing intents |
| **Exploratory** | Discover unknown failure modes through randomised or adversarial inputs | Fuzz prompts to identify jailbreak vulnerabilities |
| **Benchmark** | Measure model or chain performance against a gold-standard dataset | Compare GPT-4o vs Claude-3 Haiku on accuracy |

## Anatomy of a Test Suite

1. **Dataset** – A collection of prompts and expected behaviours
2. **Assertions** – Boolean checks expressed in Sage’s Assertion DSL (`expect.contains`, `expect.json_path`, `expect.metric < 0.05`)
3. **Scoring Function** – Optional numeric metric (BLEU, ROUGE, factuality score)
4. **Environment** – The model, chain, or endpoint under test

<Note>
Datasets can be imported from CSV/JSONL or generated on-the-fly with synthetic users.
</Note>

## Running tests

```bash title="CLI"
impact test run --suite checkout-flow --env staging
```

```python title="Python SDK"
from impact.sdk import Sage

results = (
    Sage()
      .suite("checkout-flow")
      .environment("staging")
      .run()
)
results.summary()
```

## Reporting & Gates

- **Dashboards** – Heat-maps show which intents fail and why.
- **Pull-request Checks** – Fail CI if critical assertions break.
- **Release Gates** – Block promotion to production until success rate exceeds threshold.

## Best Practices

1. **Test early & often** – Integrate Sage into your local dev workflow, not just CI.
2. **Keep datasets small but representative** – Aim for coverage, not volume.
3. **Automate synthetic data** – Use Artificial Users to cover edge-cases cheaply.
4. **Share results** – Make dashboards visible to product and compliance stakeholders.

## Further Reading

- [Assertion DSL](/api-reference/assertion-dsl)
- [Guide: Designing Effective LLM Test Suites](https://impact.ai/guides/llm-testing)
